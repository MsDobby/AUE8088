LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
  | Name     | Type             | Params
----------------------------------------------
0 | model    | ResNet           | 11.3 M
1 | loss_fn  | CrossEntropyLoss | 0
2 | accuracy | MyF1Score        | 0
----------------------------------------------
11.3 M    Trainable params
0         Non-trainable params
11.3 M    Total params
45.116    Total estimated model params size (MB)
Sanity Checking: |                                                                                                                               | 0/? [00:00<?, ?it/s][34m[1m[Val]	 root dir: /home/mmc-server4/Server/Datasets_hdd/tiny-imagenet-200/tiny-imagenet-200/val	 | # of samples: 10,000
[34m[1m[Train]	 root dir: /home/mmc-server4/Server/Datasets_hdd/tiny-imagenet-200/tiny-imagenet-200/train	 | # of samples: 100,000
Training: |                                                                                                                                      | 0/? [00:00<?, ?it/s][34m[1mACCELERATOR[39m[22m: gpu
[34m[1mBATCH_SIZE[39m[22m: 512
[34m[1mDATASET_ROOT_PATH[39m[22m: /home/mmc-server4/Server/Datasets_hdd/tiny-imagenet-200/
[34m[1mDEVICES[39m[22m: [0]
[34m[1mIMAGE_FLIP_PROB[39m[22m: 0.5
[34m[1mIMAGE_MEAN[39m[22m: [0.4802, 0.4481, 0.3975]
[34m[1mIMAGE_NUM_CROPS[39m[22m: 64
[34m[1mIMAGE_PAD_CROPS[39m[22m: 4
[34m[1mIMAGE_ROTATION[39m[22m: 20
[34m[1mIMAGE_STD[39m[22m: [0.2302, 0.2265, 0.2262]
[34m[1mMODEL_NAME[39m[22m: resnet18
[34m[1mNUM_CLASSES[39m[22m: 200
[34m[1mNUM_EPOCHS[39m[22m: 40
[34m[1mNUM_WORKERS[39m[22m: 8
[34m[1mOPTIMIZER_PARAMS[39m[22m: {'type': 'SGD', 'lr': 0.005, 'momentum': 0.9}
[34m[1mPRECISION_STR[39m[22m: 32-true
[34m[1mSCHEDULER_PARAMS[39m[22m: {'type': 'MultiStepLR', 'milestones': [30, 35], 'gamma': 0.2}
[34m[1mVAL_EVERY_N_EPOCH[39m[22m: 1
[34m[1mWANDB_ENTITY[39m[22m: ophd
[34m[1mWANDB_IMG_LOG_FREQ[39m[22m: 50
[34m[1mWANDB_NAME[39m[22m: resnet18-B512-SGD-MultiStepLR5.0E-03
[34m[1mWANDB_PROJECT[39m[22m: aue8088-pa1
[34m[1mWANDB_SAVE_DIR[39m[22m: wandb/






















































Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106]



























































Epoch 1: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=4.920, accuracy/val=0.0026, loss/train=5.140, accuracy/train=0.0101]



























































Epoch 2: 100%|████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=4.570, accuracy/val=0.00565, loss/train=4.750, accuracy/train=0.0312]



























































Epoch 3: 100%|████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=4.250, accuracy/val=0.00841, loss/train=4.420, accuracy/train=0.0579]



























































Epoch 4: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=4.030, accuracy/val=0.0108, loss/train=4.160, accuracy/train=0.0826]



























































Epoch 5: 100%|██████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=3.850, accuracy/val=0.0123, loss/train=3.940, accuracy/train=0.107]



























































Epoch 6: 100%|██████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=3.730, accuracy/val=0.0137, loss/train=3.760, accuracy/train=0.130]



























































Epoch 7: 100%|██████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=3.580, accuracy/val=0.0152, loss/train=3.620, accuracy/train=0.150]




























































Epoch 8: 100%|██████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=3.510, accuracy/val=0.0164, loss/train=3.500, accuracy/train=0.168]



























































Epoch 9: 100%|██████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.530, accuracy/val=0.0162, loss/train=3.400, accuracy/train=0.182]



























































Epoch 10: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=3.370, accuracy/val=0.0182, loss/train=3.310, accuracy/train=0.197]



























































Epoch 11: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=3.310, accuracy/val=0.0186, loss/train=3.230, accuracy/train=0.209]



























































Epoch 12: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.79it/s, v_num=y106, loss/val=3.220, accuracy/val=0.0199, loss/train=3.150, accuracy/train=0.221]



























































Epoch 13: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.200, accuracy/val=0.0201, loss/train=3.080, accuracy/train=0.233]






























































Epoch 14: 100%|█████████████████████████████| 196/196 [01:55<00:00,  1.70it/s, v_num=y106, loss/val=3.220, accuracy/val=0.0203, loss/train=3.020, accuracy/train=0.246]




























































Epoch 15: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.130, accuracy/val=0.0213, loss/train=2.960, accuracy/train=0.255]




























































Epoch 16: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.120, accuracy/val=0.0216, loss/train=2.890, accuracy/train=0.265]



























































Epoch 17: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.030, accuracy/val=0.0229, loss/train=2.840, accuracy/train=0.274]



























































Epoch 18: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=3.040, accuracy/val=0.0223, loss/train=2.780, accuracy/train=0.283]



























































Epoch 19: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=3.000, accuracy/val=0.0229, loss/train=2.720, accuracy/train=0.292]



























































Epoch 20: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.950, accuracy/val=0.0234, loss/train=2.670, accuracy/train=0.303]
































































Epoch 21: 100%|█████████████████████████████| 196/196 [01:58<00:00,  1.65it/s, v_num=y106, loss/val=2.940, accuracy/val=0.0239, loss/train=2.620, accuracy/train=0.311]



























































Epoch 22: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.930, accuracy/val=0.0241, loss/train=2.570, accuracy/train=0.321]



























































Epoch 23: 100%|█████████████████████████████| 196/196 [01:49<00:00,  1.78it/s, v_num=y106, loss/val=2.990, accuracy/val=0.0232, loss/train=2.520, accuracy/train=0.332]



























































Epoch 24: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.910, accuracy/val=0.0239, loss/train=2.460, accuracy/train=0.340]




























































Epoch 25: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.77it/s, v_num=y106, loss/val=2.890, accuracy/val=0.0245, loss/train=2.410, accuracy/train=0.349]



























































Epoch 26: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.860, accuracy/val=0.0248, loss/train=2.370, accuracy/train=0.356]
































































Epoch 27: 100%|█████████████████████████████| 196/196 [01:58<00:00,  1.65it/s, v_num=y106, loss/val=2.870, accuracy/val=0.0248, loss/train=2.320, accuracy/train=0.366]



























































Epoch 28: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.77it/s, v_num=y106, loss/val=2.940, accuracy/val=0.0243, loss/train=2.260, accuracy/train=0.379]




























































Epoch 29: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.850, accuracy/val=0.0254, loss/train=2.220, accuracy/train=0.383]



























































Epoch 30: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.870, accuracy/val=0.0254, loss/train=2.170, accuracy/train=0.395]



























































Epoch 31: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.740, accuracy/val=0.0267, loss/train=2.000, accuracy/train=0.436]



























































Epoch 32: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.750, accuracy/val=0.0266, loss/train=1.960, accuracy/train=0.443]




























































Epoch 33: 100%|██████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.730, accuracy/val=0.027, loss/train=1.940, accuracy/train=0.447]



























































Epoch 34: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.740, accuracy/val=0.0269, loss/train=1.920, accuracy/train=0.451]
































































Epoch 35: 100%|█████████████████████████████| 196/196 [01:58<00:00,  1.65it/s, v_num=y106, loss/val=2.760, accuracy/val=0.0268, loss/train=1.900, accuracy/train=0.456]



























































Epoch 36: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.77it/s, v_num=y106, loss/val=2.720, accuracy/val=0.0272, loss/train=1.860, accuracy/train=0.463]

































































Epoch 37: 100%|█████████████████████████████| 196/196 [02:00<00:00,  1.62it/s, v_num=y106, loss/val=2.720, accuracy/val=0.0271, loss/train=1.860, accuracy/train=0.464]






























































Epoch 38: 100%|██████████████████████████████| 196/196 [01:57<00:00,  1.67it/s, v_num=y106, loss/val=2.720, accuracy/val=0.027, loss/train=1.850, accuracy/train=0.466]




























































Epoch 39: 100%|█████████████████████████████| 196/196 [01:50<00:00,  1.78it/s, v_num=y106, loss/val=2.720, accuracy/val=0.0272, loss/train=1.840, accuracy/train=0.468]




Validation DataLoader 0:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████▊     | 19/20 [00:09<00:00,  1.97it/s]
`Trainer.fit` stopped: `max_epochs=40` reached.
Restoring states from the checkpoint path at wandb/aue8088-pa1/ylyry106/checkpoints/epoch=39-step=7840.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
[34m[1m[Val]	 root dir: /home/mmc-server4/Server/Datasets_hdd/tiny-imagenet-200/tiny-imagenet-200/val	 | # of samples: 10,000



Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.34it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m      Validate metric      [22m┃[1m       DataLoader 0        [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m       accuracy/val        [39m│[35m   0.027161721140146255    [39m│
│[36m         loss/val          [39m│[35m    2.7210209369659424     [39m│
└───────────────────────────┴───────────────────────────┘